{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW7yEtaWCUQL"
      },
      "source": [
        "# Overview of Prompt Engineering Best Practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8TD6U9JCUQO"
      },
      "source": [
        "## Part 1: Prompt Engineering Best Practices\n",
        "\n",
        "In this section, we provide an overview of the top tips and best practices for prompting LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byL1RG2UCUQO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMKUTILJCUQP"
      },
      "source": [
        "We first load the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCsZehLGCilc"
      },
      "outputs": [],
      "source": [
        "# ! pip install openai==0.28 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MlbaTUatCUQP"
      },
      "outputs": [],
      "source": [
        "# import openai\n",
        "import os\n",
        "import IPython\n",
        "\n",
        "# https://github.com/ollama/ollama-python/tree/main\n",
        "# import ollama\n",
        "from ollama import chat\n",
        "\n",
        "# API configuration\n",
        "# openai.api_key = \"OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F09KkfieCUQQ"
      },
      "outputs": [],
      "source": [
        "# completion function using OpenAI\n",
        "# def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300):\n",
        "#     response = openai.ChatCompletion.create(\n",
        "#         model=model,\n",
        "#         messages=messages,\n",
        "#         temperature=temperature,\n",
        "#         max_tokens=max_tokens,\n",
        "#     )\n",
        "#     return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" \\nmessages example:\\n[\\n    {\\n        'role': 'user',\\n        'content': 'Why is the sky blue?',\\n    },\\n]\\n\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# completion function using ollama\n",
        "\n",
        "def get_completion(messages, model=\"mistral\", temperature=0, max_tokens=300):\n",
        "    # response = chat(model=model, messages=messages, temperature=temperature, max_tokens=max_tokens)\n",
        "    response = chat(model=model, messages=messages)\n",
        "\n",
        "    return response['message']['content']\n",
        "\n",
        "\"\"\" \n",
        "messages example:\n",
        "[\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'Why is the sky blue?',\n",
        "    },\n",
        "]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWBwLW6kCUQR"
      },
      "source": [
        "### Be Specific and Clear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfhCFzCOCUQR"
      },
      "source": [
        "Write instructions as clear and specific as possible to get the desired LLM behaviors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vUWyqyT9CUQR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Based on the list of top global trending movies, I would recommend \"The Suicide Squad\". It is a superhero action film that features an ensemble cast of DC Comics characters. If you enjoy action-packed movies with a mix of humor and drama, then \"The Suicide Squad\" might be a great choice for you.\n"
          ]
        }
      ],
      "source": [
        "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
        "\n",
        "system_message = \"\"\"\n",
        "Your task is to recommend movies to a customer.\n",
        "\n",
        "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}.\n",
        "\n",
        "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
        "\n",
        "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
        "\"\"\"\n",
        "\n",
        "user_request = \"\"\"\n",
        "Please recommend a movie based on my interests.\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_request\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(message)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6hb7537CUQS"
      },
      "source": [
        "The more specific the desired the behavior you want from the model, the more specific the instructions and logic should be. Below is an example where the customer provides information about interests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d1sfCPwKCUQT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Based on your interest in superhero movies, I would highly recommend \"The Suicide Squad\" or \"Eternals\" for you. Both movies feature unique and compelling stories with larger-than-life characters that are sure to keep you engaged. Enjoy your movie experience!\n"
          ]
        }
      ],
      "source": [
        "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
        "\n",
        "system_message = \"\"\"\n",
        "Your task is to recommends movies to a customer.\n",
        "\n",
        "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}.\n",
        "\n",
        "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
        "\n",
        "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
        "\"\"\"\n",
        "\n",
        "user_request = \"\"\"\n",
        "I love super-hero movies. Please recommend a movie based on my interests.\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_request\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(message)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IZtFaB9CUQT"
      },
      "source": [
        "### Add Delimiters\n",
        "\n",
        "Adding delimiters help to better structure instructions and the overall prompt components. This is beneficial to get more reliable responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Gy8_gFBnCUQU"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```python In Python, you can use a list instead of an array-like data structure provided by JavaScript. Here's the equivalent code snippet in Python:\n",
              "\n",
              "```python\n",
              "strings2 = []\n",
              "strings2.append(\"one\")\n",
              "strings2.append(\"two\")\n",
              "strings2.append(\"THREE\")\n",
              "strings2.append(\"4\")\n",
              "```\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Convert the following code block in the #### <code> #### section to Python:\n",
        "\n",
        "####\n",
        "strings2.push(\"one\")\n",
        "strings2.push(\"two\")\n",
        "strings2.push(\"THREE\")\n",
        "strings2.push(\"4\")\n",
        "####\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "IPython.display.Markdown(\"```python\" + get_completion(message) + \"\\n```\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leL09Rl1CUQU"
      },
      "source": [
        "### Specify Output Format\n",
        "\n",
        "If the format of prompt responses are important, then this should be explicitly stated in the prompt to get desired results. In the example, we would like to export the results as a JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9FBIuxnKCUQU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " {\n",
            "  \"product\\_name\": \"Nike Air Max 270 React\",\n",
            "  \"product\\_brand\": \"Nike\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Your task is: given a product description, return the requested information in the section delimited by ### ###. Format the output as a JSON object.\n",
        "\n",
        "Product Description: Introducing the Nike Air Max 270 React: a comfortable and stylish sneaker that combines two of Nike's best technologies. With a sleek black design and a unique bubble sole, these shoes are perfect for everyday wear.\n",
        "\n",
        "###\n",
        "product_name: the name of the product\n",
        "product_bran: the name of the brand (if any)\n",
        "###\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "print(get_completion(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8XnZQ3kCUQU"
      },
      "source": [
        "### Think Step by Step\n",
        "\n",
        "To elicit reasoning in LLMs, you can prompt the model to think step-by-step. Prompting the model in this way allows it to provide the details steps before providing a final response that solves the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uKQz73H8CUQV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Step 1: Identify the odd numbers in the given group: 15, 32, 5, 13, 82, 7, 1.\n",
            "The odd numbers are: 1, 5, 13.\n",
            "\n",
            "Step 2: Add up the identified odd numbers:\n",
            "1 + 5 + 13 = 19\n",
            "\n",
            "Step 3: Determine if the result is odd or even:\n",
            "Since 19 is a positive integer that can be written as an even number (2 x 9) plus an odd number (1), it is itself an odd number.\n",
            "\n",
            "So, the sum of the odd numbers in this group is an odd number (19).\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response= get_completion(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYyOpLGuCUQV"
      },
      "source": [
        "### Role Playing\n",
        "\n",
        "The example below shows how to apply role playing using a chat model like GPT-3.5 Turbo/Mistral. Notice the use of system message, user message, and assistant message in the example. You can combine different messages to mimic or jump start the behavior you want or expect from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FrXEZb-MCUQV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Absolutely! A black hole is a region in space where gravity is so strong that nothing, not even light, can escape its pull when it passes the event horizon. Black holes are formed from massive stars that have exhausted their nuclear fuel and undergone a supernova explosion, causing their cores to collapse in on themselves.\n",
            "\n",
            "The process begins when a star with at least three times the mass of our sun reaches the end of its life cycle. The core collapses under the force of gravity, resulting in an extremely dense object - an object whose gravitational pull is so strong that not even light can escape it, hence the name \"black hole.\"\n",
            "\n",
            "The exact mechanism behind this collapse and the formation of the event horizon, which marks the boundary between the black hole and the rest of the universe, is still an area of ongoing research in physics. However, we have a good understanding of the basic principles involved.\n",
            "\n",
            "Black holes can be characterized by their mass, spin, and charge. They can also interact with other astronomical objects, such as stars or gas clouds, which can result in fascinating phenomena like accretion disks and jets.\n",
            "\n",
            "Is there any specific aspect of black hole formation that you'd like to know more about? Let me know if you have any questions!\n"
          ]
        }
      ],
      "source": [
        "system_message = \"\"\"\n",
        "The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\"\"\"\n",
        "\n",
        "user_message_1 = \"\"\"\n",
        "Hello, who are you?\n",
        "\"\"\"\n",
        "\n",
        "ai_message_1 = \"\"\"\n",
        "Greeting! I am an AI research assistant. How can I help you today?\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message_1\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": ai_message_1\n",
        "\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "comet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
